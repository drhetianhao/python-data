{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SXrKAeEundU"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fBiTjumxoFCRQf1zOtZUokC9mJZTw9Y6?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmx-6YifWLLQ"
      },
      "source": [
        "**NLP use cases**\n",
        "- Classifying whole sentences\n",
        "- Classifying each word in a sentence\n",
        "- Answering a question\n",
        "- Text summarization\n",
        "- Fill in the blanks\n",
        "- Translating from one language to another"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cd-h9TkLuc_6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\hetia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.24.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\hetia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hetia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\hetia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\hetia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: requests in c:\\users\\hetia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hetia\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2022.9.13)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\hetia\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.23.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hetia\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hetia\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hetia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hetia\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\hetia\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hetia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hetia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hetia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hetia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "39XhY1Y-uzMK"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=80, break_long_words=False, break_on_hyphens=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93QbwOgisBCK"
      },
      "source": [
        "##Classifying whole sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcWBiNRru7ta",
        "outputId": "55e61978-919b-4540-9dee-2576f024fe31"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1076\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1076\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m   1077\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:34\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_outputs\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     TFBaseModelOutput,\n\u001b[0;32m     28\u001b[0m     TFMaskedLMOutput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     TFTokenClassifierOutput,\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     TFMaskedLanguageModelingLoss,\n\u001b[0;32m     36\u001b[0m     TFModelInputType,\n\u001b[0;32m     37\u001b[0m     TFMultipleChoiceLoss,\n\u001b[0;32m     38\u001b[0m     TFPreTrainedModel,\n\u001b[0;32m     39\u001b[0m     TFQuestionAnsweringLoss,\n\u001b[0;32m     40\u001b[0m     TFSequenceClassificationLoss,\n\u001b[0;32m     41\u001b[0m     TFTokenClassificationLoss,\n\u001b[0;32m     42\u001b[0m     get_initializer,\n\u001b[0;32m     43\u001b[0m     keras_serializable,\n\u001b[0;32m     44\u001b[0m     unpack_inputs,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m shape_list, stable_softmax\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py:39\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m Repository, list_repo_files\n\u001b[1;32m---> 39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhdf5_format\u001b[39;00m \u001b[39mimport\u001b[39;00m save_attributes_to_hdf5_group\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m convert_file_size_to_int, get_checkpoint_shard_files\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.saving.hdf5_format'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mh:\\1代码\\NLP-Python\\tensorflow-working-with-nlp-2439112-main\\01_02 TF2_0_NLP_tasks_with_Transformers.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sentence \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mThe flights were on time both in Sydney and the connecting flight in Singapore. The organisation to cope with the COVID 19 restrictions while in transit was well planned and directions easy to follow, the plane was comfortable with a reasonable selection of in flight entertainment. Crew were pleasant and helpful.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m classifier \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39;49m\u001b[39mtext-classification\u001b[39;49m\u001b[39m'\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdistilbert-base-uncased-finetuned-sst-2-english\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m c \u001b[39m=\u001b[39m classifier(sentence)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSentence:\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\__init__.py:727\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    726\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 727\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    728\u001b[0m     model,\n\u001b[0;32m    729\u001b[0m     model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    730\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    731\u001b[0m     framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    732\u001b[0m     task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    733\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    734\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    735\u001b[0m )\n\u001b[0;32m    737\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    738\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:233\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         classes\u001b[39m.\u001b[39mappend(_class)\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m look_tf:\n\u001b[1;32m--> 233\u001b[0m     _class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(transformers_module, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTF\u001b[39;49m\u001b[39m{\u001b[39;49;00marchitecture\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m _class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m         classes\u001b[39m.\u001b[39mappend(_class)\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1067\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m   1066\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1067\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(module, name)\n\u001b[0;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1069\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1066\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m   1065\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m-> 1066\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m   1067\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1078\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1076\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m   1077\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1078\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1079\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1080\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1081\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'"
          ]
        }
      ],
      "source": [
        "sentence = 'The flights were on time both in Sydney and the connecting flight in Singapore. The organisation to cope with the COVID 19 restrictions while in transit was well planned and directions easy to follow, the plane was comfortable with a reasonable selection of in flight entertainment. Crew were pleasant and helpful.'\n",
        "classifier = pipeline('text-classification', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "c = classifier(sentence)\n",
        "print('\\nSentence:')\n",
        "print(wrapper.fill(sentence))\n",
        "print(f\"\\nThis sentence is classified with a {c[0]['label']} sentiment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvvab0H3sfgh"
      },
      "source": [
        "##Classifying each word in a sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF6vLguV6lkz",
        "outputId": "5156b21b-3901-4ec8-8c8e-06f92a1d0bf7"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1076\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1076\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m   1077\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:38\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_outputs\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     29\u001b[0m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     TFTokenClassifierOutput,\n\u001b[0;32m     37\u001b[0m )\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m     TFCausalLanguageModelingLoss,\n\u001b[0;32m     40\u001b[0m     TFMaskedLanguageModelingLoss,\n\u001b[0;32m     41\u001b[0m     TFModelInputType,\n\u001b[0;32m     42\u001b[0m     TFMultipleChoiceLoss,\n\u001b[0;32m     43\u001b[0m     TFNextSentencePredictionLoss,\n\u001b[0;32m     44\u001b[0m     TFPreTrainedModel,\n\u001b[0;32m     45\u001b[0m     TFQuestionAnsweringLoss,\n\u001b[0;32m     46\u001b[0m     TFSequenceClassificationLoss,\n\u001b[0;32m     47\u001b[0m     TFTokenClassificationLoss,\n\u001b[0;32m     48\u001b[0m     get_initializer,\n\u001b[0;32m     49\u001b[0m     keras_serializable,\n\u001b[0;32m     50\u001b[0m     unpack_inputs,\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m shape_list, stable_softmax\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py:39\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m Repository, list_repo_files\n\u001b[1;32m---> 39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhdf5_format\u001b[39;00m \u001b[39mimport\u001b[39;00m save_attributes_to_hdf5_group\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m convert_file_size_to_int, get_checkpoint_shard_files\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.saving.hdf5_format'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mh:\\1代码\\NLP-Python\\tensorflow-working-with-nlp-2439112-main\\01_02 TF2_0_NLP_tasks_with_Transformers.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSingapore Airlines was the first airline to fly the A380. Chew Choon Seng was Singapore Airline\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms CEO at the time. Singapore Airlines flies to New York daily.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ner \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39;49m\u001b[39mtoken-classification\u001b[39;49m\u001b[39m'\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdbmdz/bert-large-cased-finetuned-conll03-english\u001b[39;49m\u001b[39m'\u001b[39;49m, grouped_entities\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ners \u001b[39m=\u001b[39m ner(sentence)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSentence:\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\__init__.py:727\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    726\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 727\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    728\u001b[0m     model,\n\u001b[0;32m    729\u001b[0m     model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    730\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    731\u001b[0m     framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    732\u001b[0m     task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    733\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    734\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    735\u001b[0m )\n\u001b[0;32m    737\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    738\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:233\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         classes\u001b[39m.\u001b[39mappend(_class)\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m look_tf:\n\u001b[1;32m--> 233\u001b[0m     _class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(transformers_module, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTF\u001b[39;49m\u001b[39m{\u001b[39;49;00marchitecture\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m _class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m         classes\u001b[39m.\u001b[39mappend(_class)\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1067\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m   1066\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1067\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(module, name)\n\u001b[0;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1069\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1066\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m   1065\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m-> 1066\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m   1067\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1078\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1076\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m   1077\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1078\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1079\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1080\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1081\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'"
          ]
        }
      ],
      "source": [
        "sentence = \"Singapore Airlines was the first airline to fly the A380. Chew Choon Seng was Singapore Airline's CEO at the time. Singapore Airlines flies to New York daily.\"\n",
        "ner = pipeline('token-classification', model='dbmdz/bert-large-cased-finetuned-conll03-english', grouped_entities=True)\n",
        "ners = ner(sentence)\n",
        "print('\\nSentence:')\n",
        "print(wrapper.fill(sentence))\n",
        "print('\\n')\n",
        "for n in ners:\n",
        "  print(f\"{n['word']} -> {n['entity_group']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGGSYtXLtF_S"
      },
      "source": [
        "##Answering a question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrEtUj8BtF_S",
        "outputId": "6bd13fdc-68b7-47bb-cd96-06f834a723b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text:\n",
            " Singapore Airlines was founded in 1947 and was originally known as Malayan\n",
            "Airways. It is the national airline of Singapore and is based at Singapore\n",
            "Changi Airport.  From this hub, the airline flies to more than 60 destinations,\n",
            "with flights to Seoul, Tokyo and Melbourne among the most popular of its routes.\n",
            "It is particularly strong in Southeast Asian and Australian destinations (the\n",
            "so-called Kangaroo Route), but also flies to 6 different continents, covering 35\n",
            "countries. There are more than 100 planes in the Singapore Airlines fleet, most\n",
            "of which are Airbus aircraft plus a smaller amount Boeings. The company is known\n",
            "for frequently updating the aircraft in its fleet.\n",
            "\n",
            "Question:\n",
            "How many aircrafts does Singapore Airlines have?\n"
          ]
        }
      ],
      "source": [
        "context = '''\n",
        "Singapore Airlines was founded in 1947 and was originally known as Malayan Airways. It is the national airline of Singapore and is based at Singapore Changi Airport. \n",
        "From this hub, the airline flies to more than 60 destinations, with flights to Seoul, Tokyo and Melbourne among the most popular of its routes. \n",
        "It is particularly strong in Southeast Asian and Australian destinations (the so-called Kangaroo Route), but also flies to 6 different continents, covering 35 countries.\n",
        "There are more than 100 planes in the Singapore Airlines fleet, most of which are Airbus aircraft plus a smaller amount Boeings.\n",
        "The company is known for frequently updating the aircraft in its fleet.'''\n",
        "\n",
        "\n",
        "question = 'How many aircrafts does Singapore Airlines have?'\n",
        "\n",
        "print('Text:')\n",
        "print(wrapper.fill(context))\n",
        "print('\\nQuestion:')\n",
        "print(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "DPUX75hiDdlU",
        "outputId": "024db06f-8618-44dc-a9ea-66ede64948af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 473/473 [00:00<00:00, 490kB/s]\n",
            "c:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:125: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hetia\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1076\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1076\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m   1077\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:34\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_outputs\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     TFBaseModelOutput,\n\u001b[0;32m     28\u001b[0m     TFMaskedLMOutput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     TFTokenClassifierOutput,\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     TFMaskedLanguageModelingLoss,\n\u001b[0;32m     36\u001b[0m     TFModelInputType,\n\u001b[0;32m     37\u001b[0m     TFMultipleChoiceLoss,\n\u001b[0;32m     38\u001b[0m     TFPreTrainedModel,\n\u001b[0;32m     39\u001b[0m     TFQuestionAnsweringLoss,\n\u001b[0;32m     40\u001b[0m     TFSequenceClassificationLoss,\n\u001b[0;32m     41\u001b[0m     TFTokenClassificationLoss,\n\u001b[0;32m     42\u001b[0m     get_initializer,\n\u001b[0;32m     43\u001b[0m     keras_serializable,\n\u001b[0;32m     44\u001b[0m     unpack_inputs,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m shape_list, stable_softmax\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py:39\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m Repository, list_repo_files\n\u001b[1;32m---> 39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhdf5_format\u001b[39;00m \u001b[39mimport\u001b[39;00m save_attributes_to_hdf5_group\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m convert_file_size_to_int, get_checkpoint_shard_files\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.saving.hdf5_format'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mh:\\1代码\\NLP-Python\\tensorflow-working-with-nlp-2439112-main\\01_02 TF2_0_NLP_tasks_with_Transformers.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m qa \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39;49m\u001b[39mquestion-answering\u001b[39;49m\u001b[39m'\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdistilbert-base-cased-distilled-squad\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mQuestion:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(question \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\__init__.py:727\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    726\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 727\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    728\u001b[0m     model,\n\u001b[0;32m    729\u001b[0m     model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    730\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    731\u001b[0m     framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    732\u001b[0m     task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    733\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    734\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    735\u001b[0m )\n\u001b[0;32m    737\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    738\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:233\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         classes\u001b[39m.\u001b[39mappend(_class)\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m look_tf:\n\u001b[1;32m--> 233\u001b[0m     _class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(transformers_module, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTF\u001b[39;49m\u001b[39m{\u001b[39;49;00marchitecture\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m _class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m         classes\u001b[39m.\u001b[39mappend(_class)\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1067\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m   1066\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1067\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(module, name)\n\u001b[0;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1069\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1066\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m   1065\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m-> 1066\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m   1067\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1078\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1076\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m   1077\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1078\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1079\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1080\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1081\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa = pipeline('question-answering', model='distilbert-base-cased-distilled-squad')\n",
        "\n",
        "print('\\nQuestion:')\n",
        "print(question + '\\n')\n",
        "print('Answer:')\n",
        "a = qa(context=context, question=question)\n",
        "a['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os4TWQ9XAQgk"
      },
      "source": [
        "##Text summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVbliCgwtF_T",
        "outputId": "029942bf-c4c7-4081-c702-dcbcaede1158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Original text:\n",
            "\n",
            " Extremely unusual time to fly as we needed an exemption to fly out of Australia\n",
            "from the government. We obtained one as working in Tokyo for the year as\n",
            "teachers. The check in procedure does take a lot longer as more paperwork and\n",
            "phone calls are needed to check if you are allowed to travel. The staff were\n",
            "excellent in explaining the procedure as they are working with very few numbers.\n",
            "The flight had 40 people only, so lots of room and yes we had 3 seats each. The\n",
            "service of meals and beverages was done very quickly and efficiently. Changi\n",
            "airport was like a ghost town with most shops closed and all passengers are\n",
            "walked/transported to a transit zone until your next flight is ready. You are\n",
            "then walked in single file or transported to your next flight, so very strange\n",
            "as at times their seemed be more workers in PPE gear than passengers. The steps\n",
            "we went through at Narita were extensive, downloading apps, fill in paperwork\n",
            "and giving a saliva sample to test for covid 19.  It took about 2 hours to get\n",
            "through the steps and we only sat down for maybe 10 minutes at the last stop to\n",
            "get back your covid results.  The people involved were fantastic and we were\n",
            "lucky that we were numbers two and three in the initial first line up, but still\n",
            "over 2 hours it took so be aware. We knew we were quick as the people picking us\n",
            "up told us we were first out.\n",
            "\n",
            "Summarized text:\n",
            " The flight had 40 people only, so lots of room and yes we had 3 seats each .\n",
            "The check in procedure does take a lot longer as more paperwork and phone calls\n",
            "are needed to check if you are allowed to travel . The staff were excellent in\n",
            "explaining the procedure as they are working with very few numbers .\n"
          ]
        }
      ],
      "source": [
        "review = '''\n",
        "Extremely unusual time to fly as we needed an exemption to fly out of Australia from the government. We obtained one as working in Tokyo for the year as teachers.\n",
        "The check in procedure does take a lot longer as more paperwork and phone calls are needed to check if you are allowed to travel. The staff were excellent in explaining the procedure as they are working with very few numbers.\n",
        "The flight had 40 people only, so lots of room and yes we had 3 seats each. The service of meals and beverages was done very quickly and efficiently.\n",
        "Changi airport was like a ghost town with most shops closed and all passengers are walked/transported to a transit zone until your next flight is ready. You are then walked in single file or transported to your next flight, so very strange as at times their seemed be more workers in PPE gear than passengers.\n",
        "The steps we went through at Narita were extensive, downloading apps, fill in paperwork and giving a saliva sample to test for covid 19. \n",
        "It took about 2 hours to get through the steps and we only sat down for maybe 10 minutes at the last stop to get back your covid results. \n",
        "The people involved were fantastic and we were lucky that we were numbers two and three in the initial first line up, but still over 2 hours it took so be aware. We knew we were quick as the people picking us up told us we were first out.'''\n",
        "\n",
        "print('\\nOriginal text:\\n')\n",
        "print(wrapper.fill(review))\n",
        "summarize = pipeline('summarization', model='sshleifer/distilbart-cnn-12-6')\n",
        "summarized_text = summarize(review)[0]['summary_text']\n",
        "print('\\nSummarized text:')\n",
        "print(wrapper.fill(summarized_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6WQFMF0v1Ts"
      },
      "source": [
        "##Fill in the blanks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YgMzrVGxrGq",
        "outputId": "9f3490d3-7c5d-4f6e-d413-0ad6a6130c09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is the national anthem of Singapore\n",
            "It is the national capital of Singapore\n",
            "It is the national pride of Singapore\n",
            "It is the national treasure of Singapore\n",
            "It is the national motto of Singapore\n"
          ]
        }
      ],
      "source": [
        "sentence = 'It is the national <mask> of Singapore'\n",
        "mask = pipeline('fill-mask', model='distilroberta-base')\n",
        "masks = mask(sentence)\n",
        "for m in masks:\n",
        "  print(m['sequence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj63RtEuNl0Z",
        "outputId": "423c1585-a7ee-440a-d82a-e380187741fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Singapore Airlines is the national airline of Singapore\n",
            "Singapore Airlines is the national carrier of Singapore\n",
            "Singapore Airlines is the national airport of Singapore\n",
            "Singapore Airlines is the national airlines of Singapore\n",
            "Singapore Airlines is the national capital of Singapore\n"
          ]
        }
      ],
      "source": [
        "sentence = 'Singapore Airlines is the national <mask> of Singapore'\n",
        "mask = pipeline('fill-mask', model='distilroberta-base')\n",
        "masks = mask(sentence)\n",
        "for m in masks:\n",
        "  print(m['sequence'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7xGy6XkBUSt"
      },
      "source": [
        "##Translation (English to German)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuylAnUBCQX7",
        "outputId": "40c7f21a-5316-411c-de9b-6b4c06c27e94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 1.20k/1.20k [00:00<00:00, 1.26MB/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.t5.modeling_tf_t5 because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1076\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1076\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m   1077\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:35\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_outputs\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     TFBaseModelOutput,\n\u001b[0;32m     31\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     32\u001b[0m     TFSeq2SeqLMOutput,\n\u001b[0;32m     33\u001b[0m     TFSeq2SeqModelOutput,\n\u001b[0;32m     34\u001b[0m )\n\u001b[1;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     36\u001b[0m     TFCausalLanguageModelingLoss,\n\u001b[0;32m     37\u001b[0m     TFModelInputType,\n\u001b[0;32m     38\u001b[0m     TFPreTrainedModel,\n\u001b[0;32m     39\u001b[0m     get_initializer,\n\u001b[0;32m     40\u001b[0m     keras_serializable,\n\u001b[0;32m     41\u001b[0m     unpack_inputs,\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m shape_list, stable_softmax\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py:39\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m Repository, list_repo_files\n\u001b[1;32m---> 39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhdf5_format\u001b[39;00m \u001b[39mimport\u001b[39;00m save_attributes_to_hdf5_group\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m convert_file_size_to_int, get_checkpoint_shard_files\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.saving.hdf5_format'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mh:\\1代码\\NLP-Python\\tensorflow-working-with-nlp-2439112-main\\01_02 TF2_0_NLP_tasks_with_Transformers.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m english \u001b[39m=\u001b[39m \u001b[39m'''\u001b[39m\u001b[39mIt took about 2 hours to get through the steps and we only sat down for maybe 10 minutes at the last stop to get back your covid results. \u001b[39m\u001b[39m'''\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m translator \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39;49m\u001b[39mtranslation_en_to_de\u001b[39;49m\u001b[39m'\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mt5-base\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m german \u001b[39m=\u001b[39m translator(english)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/1%E4%BB%A3%E7%A0%81/NLP-Python/tensorflow-working-with-nlp-2439112-main/01_02%20TF2_0_NLP_tasks_with_Transformers.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEnglish:\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\__init__.py:727\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    726\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 727\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    728\u001b[0m     model,\n\u001b[0;32m    729\u001b[0m     model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    730\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    731\u001b[0m     framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    732\u001b[0m     task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    733\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    734\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    735\u001b[0m )\n\u001b[0;32m    737\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    738\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:257\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m     )\n\u001b[0;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 257\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39mfrom_pretrained(model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    258\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    259\u001b[0m         model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:462\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    459\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    460\u001b[0m     )\n\u001b[0;32m    461\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m--> 462\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_model_mapping)\n\u001b[0;32m    463\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    465\u001b[0m     )\n\u001b[0;32m    466\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:359\u001b[0m, in \u001b[0;36m_get_model_class\u001b[1;34m(config, model_mapping)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[1;32m--> 359\u001b[0m     supported_models \u001b[39m=\u001b[39m model_mapping[\u001b[39mtype\u001b[39;49m(config)]\n\u001b[0;32m    360\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(supported_models, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m    361\u001b[0m         \u001b[39mreturn\u001b[39;00m supported_models\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:590\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[39mif\u001b[39;00m model_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_mapping:\n\u001b[0;32m    589\u001b[0m     model_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_mapping[model_type]\n\u001b[1;32m--> 590\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_attr_from_module(model_type, model_name)\n\u001b[0;32m    592\u001b[0m \u001b[39m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[0;32m    593\u001b[0m model_types \u001b[39m=\u001b[39m [k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config_mapping\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m v \u001b[39m==\u001b[39m key\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[1;34m(self, model_type, attr)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[39mif\u001b[39;00m module_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules:\n\u001b[0;32m    603\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules[module_name] \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtransformers.models\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 604\u001b[0m \u001b[39mreturn\u001b[39;00m getattribute_from_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_modules[module_name], attr)\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:553\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[1;34m(module, attr)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(attr, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    552\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m attr)\n\u001b[1;32m--> 553\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39;49m(module, attr):\n\u001b[0;32m    554\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(module, attr)\n\u001b[0;32m    555\u001b[0m \u001b[39m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[39m# object at the top level.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1066\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m   1065\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m-> 1066\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m   1067\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\hetia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1078\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1076\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m   1077\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1078\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1079\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1080\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1081\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.t5.modeling_tf_t5 because of the following error (look up to see its traceback):\nNo module named 'keras.saving.hdf5_format'"
          ]
        }
      ],
      "source": [
        "english = '''It took about 2 hours to get through the steps and we only sat down for maybe 10 minutes at the last stop to get back your covid results. '''\n",
        "\n",
        "translator = pipeline('translation_en_to_de', model='t5-base')\n",
        "german = translator(english)\n",
        "print('\\nEnglish:')\n",
        "print(english)\n",
        "print('\\nGerman:')\n",
        "print(german[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                      Version\n",
            "---------------------------- ---------\n",
            "absl-py                      1.3.0\n",
            "asttokens                    2.0.5\n",
            "astunparse                   1.6.3\n",
            "autopep8                     1.7.0\n",
            "backcall                     0.2.0\n",
            "black                        22.10.0\n",
            "cachetools                   5.2.0\n",
            "certifi                      2022.9.24\n",
            "charset-normalizer           2.1.1\n",
            "click                        8.1.3\n",
            "colorama                     0.4.4\n",
            "contourpy                    1.0.5\n",
            "cycler                       0.11.0\n",
            "debugpy                      1.5.1\n",
            "decorator                    5.1.1\n",
            "entrypoints                  0.4\n",
            "executing                    0.8.3\n",
            "filelock                     3.8.0\n",
            "flatbuffers                  22.10.26\n",
            "fonttools                    4.37.4\n",
            "gast                         0.4.0\n",
            "google-auth                  2.14.1\n",
            "google-auth-oauthlib         0.4.6\n",
            "google-pasta                 0.2.0\n",
            "grpcio                       1.50.0\n",
            "h5py                         3.7.0\n",
            "huggingface-hub              0.11.0\n",
            "idna                         3.4\n",
            "ipykernel                    6.9.2\n",
            "ipython                      8.1.1\n",
            "jedi                         0.18.1\n",
            "joblib                       1.2.0\n",
            "jupyter-client               7.1.2\n",
            "jupyter-core                 4.9.2\n",
            "keras                        2.11.0\n",
            "kiwisolver                   1.4.4\n",
            "libclang                     14.0.6\n",
            "Markdown                     3.4.1\n",
            "MarkupSafe                   2.1.1\n",
            "matplotlib                   3.6.1\n",
            "matplotlib-inline            0.1.3\n",
            "mypy-extensions              0.4.3\n",
            "nest-asyncio                 1.5.4\n",
            "nltk                         3.7\n",
            "numpy                        1.23.4\n",
            "oauthlib                     3.2.2\n",
            "opt-einsum                   3.3.0\n",
            "packaging                    21.3\n",
            "pandas                       1.5.0\n",
            "parso                        0.8.3\n",
            "pathspec                     0.10.1\n",
            "pickleshare                  0.7.5\n",
            "Pillow                       9.2.0\n",
            "pip                          22.3.1\n",
            "platformdirs                 2.5.2\n",
            "prompt-toolkit               3.0.28\n",
            "protobuf                     3.19.6\n",
            "psutil                       5.9.0\n",
            "pure-eval                    0.2.2\n",
            "pyasn1                       0.4.8\n",
            "pyasn1-modules               0.2.8\n",
            "pycodestyle                  2.9.1\n",
            "Pygments                     2.11.2\n",
            "pyparsing                    3.0.9\n",
            "python-dateutil              2.8.2\n",
            "pytz                         2022.4\n",
            "pywin32                      303\n",
            "PyYAML                       6.0\n",
            "pyzmq                        22.3.0\n",
            "regex                        2022.9.13\n",
            "requests                     2.28.1\n",
            "requests-oauthlib            1.3.1\n",
            "rsa                          4.9\n",
            "scikit-learn                 1.1.3\n",
            "scipy                        1.9.3\n",
            "setuptools                   63.2.0\n",
            "six                          1.16.0\n",
            "stack-data                   0.2.0\n",
            "tensorboard                  2.11.0\n",
            "tensorboard-data-server      0.6.1\n",
            "tensorboard-plugin-wit       1.8.1\n",
            "tensorflow                   2.11.0\n",
            "tensorflow-estimator         2.11.0\n",
            "tensorflow-intel             2.11.0\n",
            "tensorflow-io-gcs-filesystem 0.27.0\n",
            "termcolor                    2.1.1\n",
            "threadpoolctl                3.1.0\n",
            "tokenize-rt                  5.0.0\n",
            "tokenizers                   0.13.2\n",
            "toml                         0.10.2\n",
            "tomli                        2.0.1\n",
            "tornado                      6.1\n",
            "tqdm                         4.64.1\n",
            "traitlets                    5.1.1\n",
            "transformers                 4.24.0\n",
            "typing_extensions            4.4.0\n",
            "urllib3                      1.26.12\n",
            "wcwidth                      0.2.5\n",
            "Werkzeug                     2.2.2\n",
            "wheel                        0.38.4\n",
            "wordcloud                    1.8.2.2\n",
            "wrapt                        1.14.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip list"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TF2.0 NLP tasks with Transformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "229e54de5b9bd0fece32b7eb2eb39c6fb994380e14030110a6535bffbb34dc1a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
