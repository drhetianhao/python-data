{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_01 Tokenization\n",
    "\n",
    "Tokenization refers to converting a text string into individual tokens. Tokens may be words or punctations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token List :  ['ÿþP\\x00R\\x00E\\x00F\\x00A\\x00C\\x00E\\x00.\\x00', '\\x00', '\\x00', '\\x00', '\\x00F\\x00r\\x00o\\x00m\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00t\\x00i\\x00m\\x00e\\x00', '\\x00o\\x00f\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00c\\x00o\\x00m\\x00i\\x00n\\x00g\\x00', '\\x00o\\x00f\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00F\\x00i\\x00r\\x00s\\x00t\\x00', '\\x00P\\x00a\\x00t\\x00r\\x00i\\x00a\\x00r\\x00c\\x00h\\x00', '\\x00B\\x00o\\x00d\\x00h\\x00i\\x00d\\x00h\\x00a\\x00r\\x00m\\x00a\\x00', '\\x00w\\x00h\\x00o\\x00', '\\x00', '\\x00t\\x00r\\x00a\\x00n\\x00s\\x00m\\x00i\\x00t\\x00t\\x00e\\x00d\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00\\x18']\n",
      "\n",
      " Total Tokens :  47777\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "\n",
    "# Read the base file into a raw text variable\n",
    "base_file = open(os.getcwd() + \"/TJ.txt\", \"rt\")\n",
    "raw_text = base_file.read()\n",
    "base_file.close()\n",
    "\n",
    "# Extract tokens\n",
    "token_list = nltk.word_tokenize(raw_text)\n",
    "print(\"Token List : \", token_list[:20])\n",
    "print(\"\\n Total Tokens : \", len(token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_02 Cleansing Text\n",
    "\n",
    "We will see examples of removing punctuation and converting to lower case\n",
    "\n",
    "#### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token List after removing punctuation :  ['ÿþP\\x00R\\x00E\\x00F\\x00A\\x00C\\x00E\\x00.\\x00', '\\x00F\\x00r\\x00o\\x00m\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00t\\x00i\\x00m\\x00e\\x00', '\\x00o\\x00f\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00c\\x00o\\x00m\\x00i\\x00n\\x00g\\x00', '\\x00o\\x00f\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00F\\x00i\\x00r\\x00s\\x00t\\x00', '\\x00P\\x00a\\x00t\\x00r\\x00i\\x00a\\x00r\\x00c\\x00h\\x00', '\\x00B\\x00o\\x00d\\x00h\\x00i\\x00d\\x00h\\x00a\\x00r\\x00m\\x00a\\x00', '\\x00w\\x00h\\x00o\\x00', '\\x00t\\x00r\\x00a\\x00n\\x00s\\x00m\\x00i\\x00t\\x00t\\x00e\\x00d\\x00', '\\x00t\\x00h\\x00e\\x00', 'W\\x00e\\x00s\\x00t\\x00e\\x00r\\x00n\\x00', \"\\x00M\\x00e\\x00s\\x00s\\x00a\\x00g\\x00e\\x00'\\x00\", '\\x00m\\x00e\\x00s\\x00s\\x00a\\x00g\\x00e\\x00', '\\x00f\\x00r\\x00o\\x00m\\x00', '\\x00I\\x00n\\x00d\\x00i\\x00a\\x00']\n",
      "\n",
      "Total tokens after removing punctuation :  36073\n"
     ]
    }
   ],
   "source": [
    "# Use the Punkt library to extract tokens\n",
    "token_list2 = list(\n",
    "    filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list)\n",
    ")\n",
    "print(\"Token List after removing punctuation : \", token_list2[:20])\n",
    "print(\"\\nTotal tokens after removing punctuation : \", len(token_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after converting to lower case :  ['ÿþp\\x00r\\x00e\\x00f\\x00a\\x00c\\x00e\\x00.\\x00', '\\x00f\\x00r\\x00o\\x00m\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00t\\x00i\\x00m\\x00e\\x00', '\\x00o\\x00f\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00c\\x00o\\x00m\\x00i\\x00n\\x00g\\x00', '\\x00o\\x00f\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00f\\x00i\\x00r\\x00s\\x00t\\x00', '\\x00p\\x00a\\x00t\\x00r\\x00i\\x00a\\x00r\\x00c\\x00h\\x00', '\\x00b\\x00o\\x00d\\x00h\\x00i\\x00d\\x00h\\x00a\\x00r\\x00m\\x00a\\x00', '\\x00w\\x00h\\x00o\\x00', '\\x00t\\x00r\\x00a\\x00n\\x00s\\x00m\\x00i\\x00t\\x00t\\x00e\\x00d\\x00', '\\x00t\\x00h\\x00e\\x00', 'w\\x00e\\x00s\\x00t\\x00e\\x00r\\x00n\\x00', \"\\x00m\\x00e\\x00s\\x00s\\x00a\\x00g\\x00e\\x00'\\x00\", '\\x00m\\x00e\\x00s\\x00s\\x00a\\x00g\\x00e\\x00', '\\x00f\\x00r\\x00o\\x00m\\x00', '\\x00i\\x00n\\x00d\\x00i\\x00a\\x00']\n",
      "\n",
      "Total tokens after converting to lower case :  36073\n"
     ]
    }
   ],
   "source": [
    "token_list3 = [word.lower() for word in token_list2]\n",
    "print(\"Token list after converting to lower case : \", token_list3[:20])\n",
    "print(\"\\nTotal tokens after converting to lower case : \", len(token_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_03 Stop word Removal\n",
    "\n",
    "Removing stop words by using a standard stop word list available in NLTK for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hetia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after removing stop words :  ['ÿþp\\x00r\\x00e\\x00f\\x00a\\x00c\\x00e\\x00.\\x00', '\\x00f\\x00r\\x00o\\x00m\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00t\\x00i\\x00m\\x00e\\x00', '\\x00o\\x00f\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00c\\x00o\\x00m\\x00i\\x00n\\x00g\\x00', '\\x00o\\x00f\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00f\\x00i\\x00r\\x00s\\x00t\\x00', '\\x00p\\x00a\\x00t\\x00r\\x00i\\x00a\\x00r\\x00c\\x00h\\x00', '\\x00b\\x00o\\x00d\\x00h\\x00i\\x00d\\x00h\\x00a\\x00r\\x00m\\x00a\\x00', '\\x00w\\x00h\\x00o\\x00', '\\x00t\\x00r\\x00a\\x00n\\x00s\\x00m\\x00i\\x00t\\x00t\\x00e\\x00d\\x00', '\\x00t\\x00h\\x00e\\x00', 'w\\x00e\\x00s\\x00t\\x00e\\x00r\\x00n\\x00', \"\\x00m\\x00e\\x00s\\x00s\\x00a\\x00g\\x00e\\x00'\\x00\", '\\x00m\\x00e\\x00s\\x00s\\x00a\\x00g\\x00e\\x00', '\\x00f\\x00r\\x00o\\x00m\\x00', '\\x00i\\x00n\\x00d\\x00i\\x00a\\x00']\n",
      "\n",
      "Total tokens after removing stop words :  36073\n"
     ]
    }
   ],
   "source": [
    "# Download the standard stopword list\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove stopwords\n",
    "token_list4 = list(\n",
    "    filter(lambda token: token not in stopwords.words(\"english\"), token_list3)\n",
    ")\n",
    "print(\"Token list after removing stop words : \", token_list4[:20])\n",
    "print(\"\\nTotal tokens after removing stop words : \", len(token_list4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_04 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after stemming :  ['order', 'construct', 'data', 'pipelin', 'network', 'stream', 'process', 'store', 'data', 'data', 'engin', 'data-sci', 'devop', 'specialist', 'must', 'understand', 'combin', 'multipl', 'big', 'data']\n",
      "\n",
      "Total tokens after Stemming :  62\n"
     ]
    }
   ],
   "source": [
    "# Use the PorterStemmer library for stemming.\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem data\n",
    "token_list5 = [stemmer.stem(word) for word in token_list4]\n",
    "print(\"Token list after stemming : \", token_list5[:20])\n",
    "print(\"\\nTotal tokens after Stemming : \", len(token_list5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_05 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hetia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after Lemmatization :  ['ÿþp\\x00r\\x00e\\x00f\\x00a\\x00c\\x00e\\x00.\\x00', '\\x00f\\x00r\\x00o\\x00m\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00t\\x00i\\x00m\\x00e\\x00', '\\x00o\\x00f\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00c\\x00o\\x00m\\x00i\\x00n\\x00g\\x00', '\\x00o\\x00f\\x00', '\\x00t\\x00h\\x00e\\x00', '\\x00f\\x00i\\x00r\\x00s\\x00t\\x00', '\\x00p\\x00a\\x00t\\x00r\\x00i\\x00a\\x00r\\x00c\\x00h\\x00', '\\x00b\\x00o\\x00d\\x00h\\x00i\\x00d\\x00h\\x00a\\x00r\\x00m\\x00a\\x00', '\\x00w\\x00h\\x00o\\x00', '\\x00t\\x00r\\x00a\\x00n\\x00s\\x00m\\x00i\\x00t\\x00t\\x00e\\x00d\\x00', '\\x00t\\x00h\\x00e\\x00', 'w\\x00e\\x00s\\x00t\\x00e\\x00r\\x00n\\x00', \"\\x00m\\x00e\\x00s\\x00s\\x00a\\x00g\\x00e\\x00'\\x00\", '\\x00m\\x00e\\x00s\\x00s\\x00a\\x00g\\x00e\\x00', '\\x00f\\x00r\\x00o\\x00m\\x00', '\\x00i\\x00n\\x00d\\x00i\\x00a\\x00']\n",
      "\n",
      "Total tokens after Lemmatization :  36073\n"
     ]
    }
   ],
   "source": [
    "# Use the wordnet library to map words to their lemmatized form\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4]\n",
    "print(\"Token list after Lemmatization : \", token_list6[:20])\n",
    "print(\"\\nTotal tokens after Lemmatization : \", len(token_list6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of tokens between raw, stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token_list5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-19df355a39eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Check for token technlogies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"Raw : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_list4\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" , Stemmed : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_list5\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" , Lemmatized : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_list6\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'token_list5' is not defined"
     ]
    }
   ],
   "source": [
    "# Check for token technlogies\n",
    "print(\n",
    "    \"Raw : \",\n",
    "    token_list4[20],\n",
    "    \" , Stemmed : \",\n",
    "    token_list5[20],\n",
    "    \" , Lemmatized : \",\n",
    "    token_list6[20],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
